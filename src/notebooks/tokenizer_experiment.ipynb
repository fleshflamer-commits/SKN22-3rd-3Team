{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1635bc9",
   "metadata": {},
   "source": [
    "# üß™ Tokenizer Logic Experimentation\n",
    "\n",
    "Ïù¥ ÎÖ∏Ìä∏Î∂ÅÏùÄ `src/utils/text.py`Ïùò ÌïµÏã¨ Î°úÏßÅÏùÑ **ÏßÅÏ†ë ÏàòÏ†ïÌïòÍ≥† ÌÖåÏä§Ìä∏**ÌïòÍ∏∞ ÏúÑÌï¥ ÏΩîÎìúÎ•º Í∞ÄÏ†∏Ïò® Î≤ÑÏ†ÑÏûÖÎãàÎã§.\n",
    "Ïó¨Í∏∞ÏÑú Ìï®ÏàòÎ•º ÏàòÏ†ïÌïòÍ≥† Î∞îÎ°ú ÏïÑÎûò ÏÖÄÏóêÏÑú Í≤∞Í≥ºÎ•º ÌôïÏù∏Ìï¥Î≥¥ÏÑ∏Ïöî!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b63c4684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tokenizer Code Loaded!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import ssl\n",
    "from kiwipiepy import Kiwi\n",
    "\n",
    "# SSL Context Fix for Kiwi Model Download\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "# ==========================================\n",
    "# üõ†Ô∏è Core Logic (from src/utils/text.py)\n",
    "# ==========================================\n",
    "\n",
    "# Singleton Instances\n",
    "_kiwi = None\n",
    "_stopwords = set()\n",
    "_synonyms = {}\n",
    "\n",
    "def get_kiwi():\n",
    "    global _kiwi\n",
    "    if _kiwi is None:\n",
    "        _kiwi = Kiwi()\n",
    "        # Load User Dictionary (Paths adapted for notebook location)\n",
    "        possible_paths = [\n",
    "            \"../../data/v3/domain_dictionary.txt\",  # From src/notebooks\n",
    "            \"data/v3/domain_dictionary.txt\",        # From root\n",
    "            \"../data/v3/domain_dictionary.txt\",     # From src\n",
    "            \"/Users/leemdo/Workspaces/SKN22-3rd-3Team/data/v3/domain_dictionary.txt\" # Absolute\n",
    "        ]\n",
    "        for path in possible_paths:\n",
    "            if os.path.exists(path):\n",
    "                print(f\"üìö Loading User Dictionary from: {path}\")\n",
    "                _kiwi.load_user_dictionary(path)\n",
    "                break\n",
    "    return _kiwi\n",
    "\n",
    "def load_resources():\n",
    "    \"\"\"Loads external resources (stopwords, synonyms) if not already loaded.\"\"\"\n",
    "    global _stopwords, _synonyms\n",
    "    \n",
    "    if not _stopwords:\n",
    "        paths = [\n",
    "            \"../../src/core/stopwords.txt\",\n",
    "            \"src/core/stopwords.txt\",\n",
    "            \"/Users/leemdo/Workspaces/SKN22-3rd-3Team/src/core/stopwords.txt\"\n",
    "        ]\n",
    "        for path in paths:\n",
    "            if os.path.exists(path):\n",
    "                with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    _stopwords = set(line.strip() for line in f if line.strip())\n",
    "                print(f\"üõë Loaded {len(_stopwords)} stopwords from {path}\")\n",
    "                break\n",
    "    \n",
    "    if not _synonyms:\n",
    "        paths = [\n",
    "            \"../../src/core/synonyms.json\",\n",
    "            \"src/core/synonyms.json\",\n",
    "            \"/Users/leemdo/Workspaces/SKN22-3rd-3Team/src/core/synonyms.json\"\n",
    "        ]\n",
    "        for path in paths:\n",
    "            if os.path.exists(path):\n",
    "                with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    grouped_data = json.load(f)\n",
    "                    \n",
    "                # Flatten the structure\n",
    "                for standard_name, aliases in grouped_data.items():\n",
    "                    if isinstance(aliases, list):\n",
    "                        for alias in aliases:\n",
    "                            _synonyms[alias] = standard_name\n",
    "                    elif isinstance(aliases, str):\n",
    "                        _synonyms[standard_name] = aliases\n",
    "                        \n",
    "                print(f\"üîÑ Loaded {len(_synonyms)} flattened synonym mappings from {path}\")\n",
    "                break\n",
    "\n",
    "def tokenize_korean(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Tokenizes Korean text using Kiwi (with Domain Dictionary).\n",
    "    Extracts Nouns (NN*), Verbs (VV), Adjectives (VA), Roots (XR).\n",
    "    Removes Stopwords.\n",
    "    Applies Synonyms.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "        \n",
    "    kiwi = get_kiwi()\n",
    "    load_resources()\n",
    "    \n",
    "    tokens = kiwi.tokenize(text)\n",
    "    \n",
    "    selected_tokens = []\n",
    "    for t in tokens:\n",
    "        # Filter tags: Noun, Verb, Adjective, Root\n",
    "        if t.tag.startswith(('N', 'V', 'XR')):\n",
    "            # Filter stopwords\n",
    "            if t.form not in _stopwords:\n",
    "                # Synonym Replacement\n",
    "                token_form = t.form\n",
    "                if token_form in _synonyms:\n",
    "                    token_form = _synonyms[token_form]\n",
    "                    \n",
    "                selected_tokens.append(token_form)\n",
    "    \n",
    "    return \" \".join(selected_tokens)\n",
    "\n",
    "print(\"‚úÖ Tokenizer Code Loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bde72f",
   "metadata": {},
   "source": [
    "### üß™ Test Cases\n",
    "ÏïÑÎûòÏóêÏÑú ÌÜ†ÌÅ¨ÎÇòÏù¥Ïßï Í≤∞Í≥ºÎ•º ÌôïÏù∏ÌïòÏÑ∏Ïöî."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a73ed29c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantization is not supported for ArchType::neon. Fall back to non-quantized model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Loading User Dictionary from: ../../data/v3/domain_dictionary.txt\n",
      "üõë Loaded 28 stopwords from ../../src/core/stopwords.txt\n",
      "üîÑ Loaded 46 flattened synonym mappings from ../../src/core/synonyms.json\n",
      "'Îü¨Î∏î ÌÇ§Ïö∞Í≥† Ïã∂Ïñ¥' -> Îü¨ÏãúÏïà Î∏îÎ£® ÌÇ§Ïö∞\n",
      "'ÏïÑÎ©îÏàè ÏÑ±Í≤©' -> ÏïÑÎ©îÎ¶¨Ïπ∏ ÏáºÌä∏Ìó§Ïñ¥ ÏÑ±Í≤©\n",
      "'ÏßëÏÇ¨Í∞Ä ÎßõÎèôÏÇ∞ Ï∫îÎã§' -> ÏßëÏÇ¨ ÎßõÎèôÏÇ∞ Ï∫ê\n",
      "'Í≥†ÏñëÏù¥Í∞Ä Î∞•ÏùÑ Î®πÏóàÎã§' -> Í≥†ÏñëÏù¥ Î∞• Î®π\n"
     ]
    }
   ],
   "source": [
    "# 1. Breed Names & Synonyms\n",
    "print(f\"'Îü¨Î∏î ÌÇ§Ïö∞Í≥† Ïã∂Ïñ¥' -> {tokenize_korean('Îü¨Î∏î ÌÇ§Ïö∞Í≥† Ïã∂Ïñ¥')}\")\n",
    "print(f\"'ÏïÑÎ©îÏàè ÏÑ±Í≤©' -> {tokenize_korean('ÏïÑÎ©îÏàè ÏÑ±Í≤©')}\")\n",
    "\n",
    "# 2. Slang & Jargon\n",
    "print(f\"'ÏßëÏÇ¨Í∞Ä ÎßõÎèôÏÇ∞ Ï∫îÎã§' -> {tokenize_korean('ÏßëÏÇ¨Í∞Ä ÎßõÎèôÏÇ∞ Ï∫îÎã§')}\")\n",
    "\n",
    "# 3. Stopwords\n",
    "print(f\"'Í≥†ÏñëÏù¥Í∞Ä Î∞•ÏùÑ Î®πÏóàÎã§' -> {tokenize_korean('Í≥†ÏñëÏù¥Í∞Ä Î∞•ÏùÑ Î®πÏóàÎã§')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a9b6d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
